{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d33a647c-6e21-446f-b63a-ffbbdf08edfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, feature_dim=256):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.fc = nn.Linear(128, feature_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class SeqDecoder(nn.Module):\n",
    "    def __init__(self, feature_dim, num_actions, seq_len=10, emb_dim=128):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.action_emb = nn.Embedding(num_actions + 2, emb_dim) # +2 for start and end tokens\n",
    "        self.gru = nn.GRU(emb_dim, feature_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(feature_dim, num_actions + 2)\n",
    "\n",
    "    def forward(self, features, action_seq=None, teacher_forcing_ratio=0.5):\n",
    "        batch_size = features.size(0)\n",
    "        input_token = torch.full((batch_size, 1), 0, dtype=torch.long, device=features.device) # start token idx = 0\n",
    "        outputs = []\n",
    "        hidden = features.unsqueeze(0) # (1, B, F)\n",
    "        for t in range(self.seq_len):\n",
    "            emb = self.action_emb(input_token) # (B, 1, emb_dim)\n",
    "            out, hidden = self.gru(emb, hidden)\n",
    "            logits = self.fc(out.squeeze(1)) # (B, num_actions+2)\n",
    "            outputs.append(logits)\n",
    "            if action_seq is not None and torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                input_token = action_seq[:, t].unsqueeze(1)\n",
    "            else:\n",
    "                input_token = logits.argmax(dim=-1, keepdim=True)\n",
    "        return torch.stack(outputs, dim=1) # (B, seq_len, num_actions+2)\n",
    "\n",
    "class RotSeqModel(nn.Module):\n",
    "    def __init__(self, num_actions=4, seq_len=10, feature_dim=256):\n",
    "        super().__init__()\n",
    "        self.encoder = ImageEncoder(feature_dim)\n",
    "        self.num_actions = num_actions\n",
    "        self.seq_len = seq_len\n",
    "        self.decoder = SeqDecoder(feature_dim * 2, num_actions, seq_len)\n",
    "\n",
    "    def forward(self, img_start, img_end, action_seq=None, teacher_forcing_ratio=0.5):\n",
    "        feat1 = self.encoder(img_start)\n",
    "        feat2 = self.encoder(img_end)\n",
    "        features = torch.cat([feat1, feat2], dim=-1)\n",
    "        return self.decoder(features, action_seq, teacher_forcing_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29858698-64e8-43ea-a881-2980424efb35",
   "metadata": {},
   "source": [
    "Predicting the sequence of transformations of a picture to obtain one from another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a286b59a-5e2b-4ab4-bacc-209ada3143c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def sequence_loss(pred_logits, target_seq, ignore_index=-100):\n",
    "    \"\"\"\n",
    "    pred_logits: (B, seq_len, num_tokens)\n",
    "    target_seq: (B, seq_len)\n",
    "    \"\"\"\n",
    "    loss = F.cross_entropy(\n",
    "        pred_logits.view(-1, pred_logits.size(-1)),\n",
    "        target_seq.view(-1),\n",
    "        ignore_index=ignore_index\n",
    "    )\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a326cf-a4a5-42e8-97c6-2b7fd90c5fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_start, img_end: (B, 3, H, W) tensors\n",
    "# target_seq: (B, seq_len) LongTensor with tokens\n",
    "model = RotSeqModel(num_actions=4, seq_len=6)\n",
    "logits = model(img_start, img_end, action_seq=target_seq)\n",
    "loss = sequence_loss(logits, target_seq)\n",
    "loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
